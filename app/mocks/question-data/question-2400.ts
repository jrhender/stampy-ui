export const question2400 = {
  id: 'i-0d89f207fea069e23edf50d47617c9c8f21bbf5c401e7f2578e188fb309a5774',
  type: 'row',
  href: 'https://coda.io/apis/v1/docs/fau7sl2hmG/tables/grid-sync-1059-File/rows/i-0d89f207fea069e23edf50d47617c9c8f21bbf5c401e7f2578e188fb309a5774',
  name: 'Why is AGI dangerous?',
  index: 252,
  createdAt: '2023-01-14T14:46:14.123Z',
  updatedAt: '2023-04-27T22:22:21.189Z',
  browserLink:
    'https://coda.io/d/_dfau7sl2hmG#_tugrid-sync-1059-File/_rui-0d89f207fea069e23edf50d47617c9c8f21bbf5c401e7f2578e188fb309a5774',
  values: {
    File: 'Why is AGI dangerous?',
    Synced: false,
    'Sync account': {
      '@context': 'http://schema.org/',
      '@type': 'StructuredValue',
      additionalType: 'row',
      name: 'stampysaisafetyinfo@gmail.com',
      url: 'https://coda.io/d/_dfau7sl2hmG#_tuGlobal-External-Connections/_rui-1fffa4c7-80ba-4bd0-804e-e432be8d2052',
      tableId: 'Global-External-Connections',
      rowId: 'i-1fffa4c7-80ba-4bd0-804e-e432be8d2052',
      tableUrl: 'https://coda.io/d/_dfau7sl2hmG#_tuGlobal-External-Connections',
    },
    Question: '```Why is AGI dangerous?```',
    Link: {
      '@context': 'http://schema.org/',
      '@type': 'WebPage',
      url: 'https://docs.google.com/document/d/1ItfAkZNiskwSpT20Wv_OFql2YdzSOUrZ4_2eRijdUNk/edit?usp=drivesdk',
    },
    Thumbnail: {
      '@context': 'http://schema.org/',
      '@type': 'ImageObject',
      name: 'image.jpeg',
      height: 220,
      width: 170,
      url: 'https://codahosted.io/docs/fau7sl2hmG/blobs/bl-Ax7ivng4ml/fb82d19d696f9470914aa83504c5edabca89ff2a23ac4d9f0193e2b805a639a7131e65c1720c689ea5c8315daf4db239c15f8131517c2049e70c73f2435125988d4e40934ad455efa490fb2cd029c9b8eea8d169d801509638e183d076ab92b8cdea0c98',
      status: 'live',
    },
    'Doc Created': '2023-01-14T14:54:53.674+01:00',
    'Related Answers DO NOT EDIT': [],
    Tags: '',
    'Doc Last Edited': '2023-02-27T19:41:29.859+01:00',
    Status: {
      '@context': 'http://schema.org/',
      '@type': 'StructuredValue',
      additionalType: 'row',
      name: 'Live on site',
      url: 'https://coda.io/d/_dfau7sl2hmG#_tugrid-IWDInbu5n2/_rui-7EfvxV9G0N',
      tableId: 'grid-IWDInbu5n2',
      rowId: 'i-7EfvxV9G0N',
      tableUrl: 'https://coda.io/d/_dfau7sl2hmG#_tugrid-IWDInbu5n2',
    },
    'Edit Answer':
      '**[Why is AGI dangerous?](https://docs.google.com/document/d/1ItfAkZNiskwSpT20Wv_OFql2YdzSOUrZ4_2eRijdUNk/edit?usp=drivesdk)**',
    'Alternate Phrasings': '',
    'UI ID DO NOT EDIT': '```2400```',
    'Source Link': '',
    'aisafety.info Link': '**[Why is AGI dangerous?](https://aisafety.info/?state=2400_)**',
    Source: '```Wiki```',
    'All Phrasings': '```Why is AGI dangerous?\n```',
    'Initial Order': '',
    'Related IDs': [],
    'Rich Text DO NOT EDIT':
      '```1. [The Orthogonality Thesis](https://www.youtube.com/watch?v=hEUO6pjwFOo): AI could have almost any goal while at the same time having high intelligence (aka ability to succeed at those goals). This means that we could build a very powerful agent which would not necessarily share human-friendly values. For example, the classic [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer) thought experiment explores this with an AI which has a goal of creating as many paperclips as possible, something that humans are (mostly) indifferent to, and as a side effect ends up destroying humanity to make room for more paperclip factories.\n\n1. [Complexity of value](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile): What humans care about is not simple, and the space of all goals is large, so virtually all goals we could program into an AI would lead to worlds not valuable to humans if pursued by a sufficiently powerful agent. If we, for example, did not include our value of diversity of experience, we could end up with a world of endlessly looping simple pleasures, rather than beings living rich lives.\n\n1. [Instrumental Convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q): For almost any goal an AI has there are shared ‘instrumental’ steps, such as acquiring resources, preserving itself, and preserving the contents of its goals. This means that a powerful AI with goals that were not explicitly human-friendly would predictably both take actions that lead to the end of humanity (e.g. using resources humans need to live to further its goals, such as replacing our crop fields with vast numbers of solar panels to power its growth, or using the carbon in our bodies to build things) and prevent us from turning it off or altering its goals.\n\n```',
    'Tag Count': 0,
    'Related Answer Count': 0,
    'Rich Text':
      '```1. [The Orthogonality Thesis](https://www.youtube.com/watch?v=hEUO6pjwFOo): AI could have almost any goal while at the same time having high intelligence (aka ability to succeed at those goals). This means that we could build a very powerful agent which would not necessarily share human-friendly values. For example, the classic [paperclip maximizer](https://www.lesswrong.com/tag/paperclip-maximizer) thought experiment explores this with an AI which has a goal of creating as many paperclips as possible, something that humans are (mostly) indifferent to, and as a side effect ends up destroying humanity to make room for more paperclip factories.\n\n1. [Complexity of value](https://www.lesswrong.com/posts/GNnHHmm8EzePmKzPk/value-is-fragile): What humans care about is not simple, and the space of all goals is large, so virtually all goals we could program into an AI would lead to worlds not valuable to humans if pursued by a sufficiently powerful agent. If we, for example, did not include our value of diversity of experience, we could end up with a world of endlessly looping simple pleasures, rather than beings living rich lives.\n\n1. [Instrumental Convergence](https://www.youtube.com/watch?v=ZeecOKBus3Q): For almost any goal an AI has there are shared ‘instrumental’ steps, such as acquiring resources, preserving itself, and preserving the contents of its goals. This means that a powerful AI with goals that were not explicitly human-friendly would predictably both take actions that lead to the end of humanity (e.g. using resources humans need to live to further its goals, such as replacing our crop fields with vast numbers of solar panels to power its growth, or using the carbon in our bodies to build things) and prevent us from turning it off or altering its goals.\n\n```',
    'Stamp Count': 1,
    'Multi Answer': '',
    'Stamped By': {
      '@context': 'http://schema.org/',
      '@type': 'Person',
      name: 'plex',
      email: 'plexven@gmail.com',
    },
    Priority: 4,
    Asker: '```Jack Harley```',
    'External Source': '',
    'Last Asked On Discord': '',
    'UI ID': '```2400```',
    'Related Answers': [],
    'Doc Last Ingested': '2023-04-28T00:20:06.478+02:00',
    'Request Count': '',
    'Number of suggestions on answer doc': 10,
    'Total character count of suggestions on answer doc': 140,
    Helpful: '',
  },
}
